{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import glob\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "from music21 import converter, instrument, stream, note, chord\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Activation, Bidirectional, Flatten\n",
    "from keras import utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras_self_attention import SeqSelfAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PARAMETERS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 139\n"
     ]
    }
   ],
   "source": [
    "data_path = pathlib.Path('../raw_data/midi_chopin_format_0_dataset_full')\n",
    "filenames = glob.glob(str(data_path/\"*.mid\"))\n",
    "print('Number of files:', len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PARAMETERS #####\n",
    "SEQ_LENGTH = 25                 #default 100, sequence length\n",
    "NUM_FILES = 1                 #len(filenames) for the whole dataset (number of files to use)\n",
    "PERIOD = 1                      #default 10, saving weights every xx epoch\n",
    "BATCH_SIZE = 16                 #default 64\n",
    "EPOCHS = 1                     #default 200\n",
    "\n",
    "NUM_NOTES_TO_GENERATE = 100     #default 500\n",
    "STORAGE_PATH = '../raw_data/preprocessed_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **FUNCTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_notes():\n",
    "    \"\"\" Get all the notes and chords from the midi files - Call BEFORE train \"\"\"\n",
    "    notes = []\n",
    "    \n",
    "    print(\"Parsing files...\")\n",
    "    \n",
    "    for file in filenames[:NUM_FILES]:\n",
    "        stream_file = converter.parse(file)\n",
    "\n",
    "        #print(\"Parsing %s\" % file)\n",
    "                \n",
    "        components_to_parse = []\n",
    "        for element in stream_file.recurse():\n",
    "            components_to_parse.append(element)\n",
    "        \n",
    "        #components_to_parse = stream_file.flat.notes #return to this to preview model output 10 epochs 1st training\n",
    "    \n",
    "        for element in components_to_parse:\n",
    "            if isinstance(element, note.Note):\n",
    "                notes.append(str(element.pitch) + \" \" +  str(float(element.quarterLength)))\n",
    "            elif isinstance(element, chord.Chord):\n",
    "                notes.append('.'.join(str(n) for n in element.normalOrder) + \" \" + str(float(element.quarterLength)))\n",
    "            elif isinstance(element, note.Rest):\n",
    "                notes.append(str(element.name)  + \" \" + str(float(element.quarterLength)))\n",
    "\n",
    "    with open(f\"../raw_data/notes/notes_{NUM_FILES}files.pickle\", 'wb') as f:\n",
    "        pickle.dump(notes, f)\n",
    "    \n",
    "    print(\"Parsing done.\")\n",
    "    \n",
    "    return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(notes, n_vocab):\n",
    "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
    "    sequence_length = SEQ_LENGTH\n",
    "\n",
    "\n",
    "    # get all pitch names\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "\n",
    "    # create a dictionary to map pitches to integers\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    network_input = []\n",
    "    network_output = []\n",
    "\n",
    "    # create input sequences and the corresponding outputs\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        network_output.append(note_to_int[sequence_out])\n",
    "\n",
    "    n_patterns = len(network_input)\n",
    "\n",
    "    # reshape the input into a format compatible with LSTM layers\n",
    "    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    # normalize input\n",
    "    network_input = network_input / float(n_vocab)\n",
    "    network_output = utils.to_categorical(network_output)\n",
    "    \n",
    "    #Transforming to pandas Dataframe and saving to CSV files \n",
    "    network_input_2d = np.reshape(network_input, (network_input.shape[0], -1))\n",
    "    network_input_df = pd.DataFrame(network_input_2d)\n",
    "    \n",
    "    #network_output_2d = np.reshape(network_output, (network_output.shape[0], -1))\n",
    "    network_output_df = pd.DataFrame(network_output)\n",
    "    \n",
    "    network_input_df.to_csv(STORAGE_PATH+'/network_inputs.csv',mode='w')\n",
    "    network_output_df.to_csv(STORAGE_PATH+'/network_outputs.csv', mode='w')\n",
    "\n",
    "\n",
    "    return (network_input, network_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(network_input, n_vocab):\n",
    "    \"\"\" create the structure of the neural network \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(512,\n",
    "        input_shape=(network_input.shape[1], network_input.shape[2]),\n",
    "        return_sequences=True)))\n",
    "    model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(LSTM(512,return_sequences=True))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(n_vocab))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, network_input, network_output):\n",
    "    \"\"\" train the neural network \"\"\"\n",
    "    now = datetime.now()\n",
    "    filepath = os.path.abspath(f\"model_checkpoints_weights/{now.strftime('%d-%m/%H-%M-%S')}_weights-1LSTMAtt1LSTMLayer-num_files_{NUM_FILES}-seq_length_{SEQ_LENGTH}-batch_size_{BATCH_SIZE}\"+\"-{epoch:03d}-{loss:.4f}.hdf5\")\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath,\n",
    "        period=PERIOD, #Save weights every xx epochs\n",
    "        monitor='loss',\n",
    "        verbose=1,\n",
    "        save_best_only=False,\n",
    "        mode='min'\n",
    "    )\n",
    "    callbacks_list = [checkpoint]\n",
    "\n",
    "    model.fit(network_input, network_output, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(notes, n_vocab):\n",
    "    \"\"\" Train a Neural Network to generate music \"\"\"\n",
    "    network_input, network_output = prepare_sequences(notes, n_vocab)\n",
    "\n",
    "    model = create_network(network_input, n_vocab)\n",
    "\n",
    "    train(model, network_input, network_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **TRAIN MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing files...\n",
      "Parsing done.\n",
      "n_vocab: 1417\n"
     ]
    }
   ],
   "source": [
    "#load files in\n",
    "notes = get_notes()\n",
    "\n",
    "# get amount of pitch names\n",
    "n_vocab = len(set(notes))\n",
    "print(f\"n_vocab: {n_vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rest 0.25',\n",
       " 'G#4 0.3333333333333333',\n",
       " 'rest 2.4166666666666665',\n",
       " 'B1 1.0',\n",
       " 'rest 0.25']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check notes format\n",
    "notes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5827, 25, 1)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check network_input shape\n",
    "network_input, network_output = prepare_sequences(notes, n_vocab)\n",
    "network_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5827, 1417)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 09:51:58.036858: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-12-12 09:51:58.037069: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-12 09:51:58.037168: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-12 09:51:58.037247: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-12 09:51:58.037330: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-12 09:51:58.037391: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-12-12 09:51:58.037435: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-12 09:51:58.037472: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-12-12 09:51:58.037506: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-12-12 09:51:58.037515: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-12-12 09:51:58.038621: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/chibidao/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer GlorotNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "653/653 [==============================] - ETA: 0s - loss: 6.4118\n",
      "Epoch 1: saving model to /home/chibidao/code/aeraymd/chopin_28_prelude/notebooks-vADHO/model_checkpoints_weights/12-12/09-51-58_weights-1LSTMAtt1LSTMLayer-num_files_3-seq_length_25-batch_size_16-001-6.4118.hdf5\n",
      "653/653 [==============================] - 514s 778ms/step - loss: 6.4118\n"
     ]
    }
   ],
   "source": [
    "#train model\n",
    "train_network(notes, n_vocab) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Generating music**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **MANUAL INPUT BEFORE GENERATOR STEP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path to the weights to load\n",
    "WEIGHTS_PATH = 'model_checkpoints_weights/'+'12-12/09-51-58_weights-1LSTMAtt1LSTMLayer-num_files_3-seq_length_25-batch_size_16-001-6.4118.hdf5'\n",
    "#Filename for generated midi file\n",
    "OUTPUT_FILENAME = f\"midi_output_{NUM_FILES}files\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **FUNCTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network_add_weights(network_input, n_vocab):\n",
    "    \"\"\" create the structure of the neural network \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Bidirectional(LSTM(512,\n",
    "        input_shape=(network_input.shape[1], network_input.shape[2]), #n_time_steps, n_features\n",
    "        return_sequences=True)))\n",
    "    model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(LSTM(512,return_sequences=True))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    model.add(Flatten()) #Supposedly needed to fix stuff before dense layer\n",
    "    model.add(Dense(n_vocab))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences_output(notes, pitchnames, n_vocab):\n",
    "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
    "    # map between notes and integers and back\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    sequence_length = SEQ_LENGTH\n",
    "    network_input = []\n",
    "    output = []\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        output.append(note_to_int[sequence_out])\n",
    "\n",
    "    n_patterns = len(network_input)\n",
    "\n",
    "    # reshape the input into a format compatible with LSTM layers\n",
    "    normalized_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    # normalize input\n",
    "    normalized_input = normalized_input / float(n_vocab)\n",
    "\n",
    "    return (network_input, normalized_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_notes(model, network_input, pitchnames, n_vocab, NUM_NOTES_TO_GENERATE):\n",
    "    \"\"\" Generate notes from the neural network based on a sequence of notes \"\"\"\n",
    "    # pick a random sequence from the input as a starting point for the prediction\n",
    "    start = np.random.randint(0, len(network_input)-1)\n",
    "\n",
    "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    pattern = network_input[start]\n",
    "    prediction_output = []\n",
    "\n",
    "    # generate notes\n",
    "    for note_index in range(NUM_NOTES_TO_GENERATE):\n",
    "        prediction_input = np.reshape(pattern, (1, len(pattern), 1))\n",
    "        prediction_input = prediction_input / float(n_vocab)\n",
    "\n",
    "        prediction = model.predict(prediction_input, verbose=0)\n",
    "\n",
    "        index = np.argmax(prediction)\n",
    "        result = int_to_note[index]\n",
    "        prediction_output.append(result)\n",
    "\n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "\n",
    "    return prediction_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.98003992],\n",
       "        [0.93263473],\n",
       "        [0.99301397],\n",
       "        ...,\n",
       "        [0.98103792],\n",
       "        [0.6501996 ],\n",
       "        [0.97904192]],\n",
       "\n",
       "       [[0.93263473],\n",
       "        [0.99301397],\n",
       "        [0.63772455],\n",
       "        ...,\n",
       "        [0.6501996 ],\n",
       "        [0.97904192],\n",
       "        [0.68313373]],\n",
       "\n",
       "       [[0.99301397],\n",
       "        [0.63772455],\n",
       "        [0.98003992],\n",
       "        ...,\n",
       "        [0.97904192],\n",
       "        [0.68313373],\n",
       "        [0.99051896]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.27694611],\n",
       "        [0.98453094],\n",
       "        [0.98902196],\n",
       "        ...,\n",
       "        [0.98153693],\n",
       "        [0.98003992],\n",
       "        [0.08183633]],\n",
       "\n",
       "       [[0.98453094],\n",
       "        [0.98902196],\n",
       "        [0.51047904],\n",
       "        ...,\n",
       "        [0.98003992],\n",
       "        [0.08183633],\n",
       "        [0.98153693]],\n",
       "\n",
       "       [[0.98902196],\n",
       "        [0.51047904],\n",
       "        [0.98453094],\n",
       "        ...,\n",
       "        [0.08183633],\n",
       "        [0.98153693],\n",
       "        [0.51197605]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = np.random.randint(0, len(network_input)-1)\n",
    "\n",
    "# get all pitch names\n",
    "pitchnames = sorted(set(item for item in notes))\n",
    "pitchnames\n",
    "\n",
    "int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
    "\n",
    "\n",
    "pattern = network_input[start]\n",
    "prediction_output = []\n",
    "network_input\n",
    "# # generate notes\n",
    "# for note_index in range(NUM_NOTES_TO_GENERATE):\n",
    "#     prediction_input = np.reshape(pattern, (1, len(pattern), 1))\n",
    "#     prediction_input = prediction_input / float(n_vocab)\n",
    "\n",
    "#     prediction = model.predict(prediction_input, verbose=0)\n",
    "\n",
    "#     index = np.argmax(prediction)\n",
    "#     result = int_to_note[index]\n",
    "#     prediction_output.append(result)\n",
    "\n",
    "#     pattern.append(index)\n",
    "#     pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From: https://stackoverflow.com/questions/1806278/convert-fraction-to-float\n",
    "def convert_to_float(frac_str):\n",
    "    try:\n",
    "        return float(frac_str)\n",
    "    except ValueError:\n",
    "        num, denom = frac_str.split('/')\n",
    "        try:\n",
    "            leading, num = num.split(' ')\n",
    "            whole = float(leading)\n",
    "        except ValueError:\n",
    "            whole = 0\n",
    "        frac = float(num) / float(denom)\n",
    "        return whole - frac if whole < 0 else whole + frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_midi(prediction_output):\n",
    "    \"\"\" convert the output from the prediction to notes and create a midi file\n",
    "        from the notes \"\"\"\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for pattern in prediction_output:\n",
    "        pattern = pattern.split()\n",
    "        temp = pattern[0]\n",
    "        duration = pattern[1]\n",
    "        pattern = temp\n",
    "        # pattern is a chord\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                new_note = note.Note(int(current_note))\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "        # pattern is a rest\n",
    "        elif('rest' in pattern):\n",
    "            #new_rest = note.Rest(pattern)\n",
    "            new_rest = note.Rest()\n",
    "            new_rest.offset = offset\n",
    "            new_rest.storedInstrument = instrument.Piano() #???\n",
    "            output_notes.append(new_rest)\n",
    "        # pattern is a note\n",
    "        else:\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += convert_to_float(duration)\n",
    "\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    now = datetime.now()\n",
    "    fp = '../raw_data/model_outputs/'+f\"{now.strftime('%d-%m-%Y_%H-%M-%S')}_\"+OUTPUT_FILENAME+'.mid'\n",
    "    midifile = midi_stream.write('midi', fp=fp)\n",
    "    #midi_stream.show('midi')\n",
    "    return midifile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate():\n",
    "    \"\"\" Generate a piano midi file \"\"\"\n",
    "    print(\"Generating notes...\")\n",
    "    #load the notes used to train the model\n",
    "    notes = get_notes()\n",
    "    with open(f\"../raw_data/notes/notes_{NUM_FILES}files.pickle\", 'rb') as f:\n",
    "    #with open(f\"../raw_data/notes/notes_reduced.pickle\", 'rb') as f:    \n",
    "        \n",
    "        notes = pickle.load(f)\n",
    "\n",
    "    # Get all pitch names\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "    # Get all pitch names\n",
    "    n_vocab = len(set(notes))\n",
    "\n",
    "    network_input, normalized_input = prepare_sequences_output(notes, pitchnames, n_vocab)\n",
    "    model = create_network_add_weights(normalized_input, n_vocab)\n",
    "    model.build(input_shape=(None, normalized_input.shape[1], normalized_input.shape[2])) #e.g.(None,100,1)\n",
    "    model.load_weights(WEIGHTS_PATH) # Load the weights to each node\n",
    "    prediction_output = generate_notes(model, network_input, pitchnames, n_vocab, NUM_NOTES_TO_GENERATE)\n",
    "    midifile = create_midi(prediction_output)\n",
    "    print(\"Notes generation done.\")\n",
    "    return midifile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2004\n",
      "10470\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "\n",
    "# Get all pitch names\n",
    "#pitchnames = sorted(set(item for item in notes))\n",
    "# Get all pitch names\n",
    "n_vocab = len(set(notes))\n",
    "    \n",
    "#print(pitchnames)\n",
    "print(n_vocab)\n",
    "#print(notes)\n",
    "print(len(notes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **GENERATOR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating notes...\n",
      "Parsing files...\n",
      "Parsing done.\n",
      "Notes generation done.\n"
     ]
    }
   ],
   "source": [
    "#RUN THE GENERATOR\n",
    "midifile = generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(midifile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stream = converter.parse(midifile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../raw_data/midi_chopin_format_0_dataset_full/chopin_polonaise-fantasie_61_(c)lubetsky.mid',\n",
       " '../raw_data/midi_chopin_format_0_dataset_full/chpn-p2.mid',\n",
       " '../raw_data/midi_chopin_format_0_dataset_full/51126a_ballade_op_47_no_3_a_flat_(nc)smythe.mid']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames[:NUM_FILES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <div id=\"midiPlayerDiv101203\"></div>\n",
       "                <link rel=\"stylesheet\" href=\"//cuthbertLab.github.io/music21j/css/m21.css\"\n",
       "                    type=\"text/css\" />\n",
       "                \n",
       "                <script>\n",
       "                require.config({\n",
       "                    paths: {'music21': '//cuthbertLab.github.io/music21j/src/music21'}\n",
       "                });\n",
       "                require(['music21'], function() {\n",
       "                               mp = new music21.miditools.MidiPlayer();\n",
       "                               mp.addPlayer(\"#midiPlayerDiv101203\");\n",
       "                               mp.base64Load(\"data:audio/midi;base64,TVRoZAAAAAYAAQACBABNVHJrAAAAFAD/UQMHoSAA/1gEBAIYCIgA/y8ATVRyawAAAAkA/wMAiAD/LwA=\");\n",
       "                        });\n",
       "                </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Listen to the model output\n",
    "Stream.show('midi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chibidao/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/music21/musicxml/m21ToXml.py:510: MusicXMLWarning: <music21.stream.Score 0x7f0dfd56ca30> is not well-formed; see isWellFormedNotation()\n",
      "  warnings.warn(f'{scOut} is not well-formed; see isWellFormedNotation()',\n"
     ]
    },
    {
     "ename": "SubConverterException",
     "evalue": "Cannot find a path to the 'mscore' file at /usr/bin/mscore3 -- download MuseScore",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSubConverterException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [83], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Get the music score\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mStream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/music21/stream/base.py:397\u001b[0m, in \u001b[0;36mStream.show\u001b[0;34m(self, fmt, app, **keywords)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misSorted \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautoSort:\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort()\n\u001b[0;32m--> 397\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfmt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkeywords\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/music21/base.py:2888\u001b[0m, in \u001b[0;36mMusic21Object.show\u001b[0;34m(self, fmt, app, **keywords)\u001b[0m\n\u001b[1;32m   2886\u001b[0m scClass \u001b[38;5;241m=\u001b[39m common\u001b[38;5;241m.\u001b[39mfindSubConverterForFormat(regularizedConverterFormat)\n\u001b[1;32m   2887\u001b[0m formatWriter \u001b[38;5;241m=\u001b[39m scClass()\n\u001b[0;32m-> 2888\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatWriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2889\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mregularizedConverterFormat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2890\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mapp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2891\u001b[0m \u001b[43m                         \u001b[49m\u001b[43msubformats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubformats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2892\u001b[0m \u001b[43m                         \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkeywords\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/music21/converter/subConverters.py:395\u001b[0m, in \u001b[0;36mConverterIPython.show\u001b[0;34m(self, obj, fmt, app, subformats, **keywords)\u001b[0m\n\u001b[1;32m    392\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(obj\u001b[38;5;241m.\u001b[39mscores)\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m scores:\n\u001b[0;32m--> 395\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mhelperSubConverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mhelperFormat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43msubformats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhelperSubformats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m                                  \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkeywords\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m helperSubformats[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    402\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(environLocal[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmusescoreDirectPNGPath\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/skip\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/music21/converter/subConverters.py:1138\u001b[0m, in \u001b[0;36mConverterMusicXML.write\u001b[0;34m(self, obj, fmt, fp, subformats, makeNotation, compress, **keywords)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     defaults\u001b[38;5;241m.\u001b[39mauthor \u001b[38;5;241m=\u001b[39m savedDefaultAuthor\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (subformats \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m subformats \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpdf\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m subformats)\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(environLocal[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmusescoreDirectPNGPath\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/skip\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m-> 1138\u001b[0m     outFp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunThroughMusescore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxmlFp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubformats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkeywords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m compress:\n\u001b[1;32m   1140\u001b[0m     archiveTools\u001b[38;5;241m.\u001b[39mcompressXML(xmlFp,\n\u001b[1;32m   1141\u001b[0m                              deleteOriginal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                              silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1143\u001b[0m                              strictMxlCheck\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/lewagon/lib/python3.10/site-packages/music21/converter/subConverters.py:972\u001b[0m, in \u001b[0;36mConverterMusicXML.runThroughMusescore\u001b[0;34m(self, fp, subformats, dpi, **keywords)\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SubConverterException(\n\u001b[1;32m    969\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTo create PNG files directly from MusicXML you need to download MuseScore and \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    970\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mput a link to it in your .music21rc via Environment.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    971\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m musescorePath\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m--> 972\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SubConverterException(\n\u001b[1;32m    973\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find a path to the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmscore\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m file at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    974\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmusescorePath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -- download MuseScore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m subformats \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m     subformatExtension \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mSubConverterException\u001b[0m: Cannot find a path to the 'mscore' file at /usr/bin/mscore3 -- download MuseScore"
     ]
    }
   ],
   "source": [
    "# Get the music score\n",
    "Stream.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "a8a7570021751e1e44b5cb6f7d3a7e2449aa165f08b1cd960c3a5a6b6868a712"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
